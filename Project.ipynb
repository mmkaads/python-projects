{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced0f31c-2fea-48b2-a43c-cc3dc8c58b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "acbaf81c-5f2d-4e84-b0dd-c0d8a48c99d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1190, 12)\n",
      "age                    0\n",
      "sex                    0\n",
      "chest pain type        0\n",
      "resting bp s           0\n",
      "cholesterol            0\n",
      "fasting blood sugar    0\n",
      "resting ecg            0\n",
      "max heart rate         0\n",
      "exercise angina        0\n",
      "oldpeak                0\n",
      "ST slope               0\n",
      "target                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "X = data.drop('target', axis=1)  # X is all the data set except the target column\n",
    "y = data['target']                  # y is the target column\n",
    "\n",
    "print(data.isnull().sum()) # since there is no null we good to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a2d61d32-877b-48ca-89c8-1b847e58f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24489796 1.         0.33333333 ... 0.29545455 0.33333333 0.        ]\n",
      " [0.42857143 0.         0.66666667 ... 0.40909091 0.66666667 1.        ]\n",
      " [0.18367347 1.         0.33333333 ... 0.29545455 0.33333333 0.        ]\n",
      " ...\n",
      " [0.59183673 1.         1.         ... 0.43181818 0.66666667 1.        ]\n",
      " [0.59183673 0.         0.33333333 ... 0.29545455 0.66666667 1.        ]\n",
      " [0.20408163 1.         0.66666667 ... 0.29545455 0.33333333 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler # this is normalization which sets number between ( 0 and 1)\n",
    "\n",
    "#we can normalize the big numbers ,  features like 'resting bp s' , 'cholesterol' , and  'max heart rate'\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "#reshape(-1,1) ensures that each value has a row of its own\n",
    "\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "41920fff-ca06-42ad-8d7d-324c08758a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# test_size = 0.5, using 50% of data for testing and 50% for training \n",
    "# random_state, ensures that it's not shuffling every time running code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d172e56b-2a7d-4bd8-96b7-6bdbcb16e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "cls = GaussianNB()      # initialize Naive bayes  model\n",
    "\n",
    "cls.fit(X_train, Y_train)  # fit the model\n",
    "\n",
    "y_pred = cls.predict(X_test)  # use X_test to predict\n",
    "\n",
    "accuracyOfNaiveBayes = accuracy_score(Y_test, y_pred)  # use y test to check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ada6640-200a-4041-b84f-a220fbec2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 0.8487394957983193\n",
      "Descision Tree 0.8504201680672269\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#NOW LETS CHECK THE ACCURACY FOR DECISION TREE\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='gini')\n",
    "tree.fit(X_train , Y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracyOfTree = accuracy_score(Y_test,y_pred)\n",
    "\n",
    "\n",
    "print(f\"Naive Bayes:\",accuracyOfNaiveBayes) #ACCURACY IS 0.848... \n",
    "print(f\"Descision Tree:\" ,accuracyOfTree)       #Accuracy of Tree is slightly better 0.8521008\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7e24b02b-4041-4799-ad11-a9bc899478da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'gini', 'max_depth': 40, 'max_features': 'log2', 'min_samples_leaf': 1}\n",
      "Best Score:  0.8848739495798318\n"
     ]
    }
   ],
   "source": [
    "#WE CAN YOU GRID SEARCH TO TUNE HYPER PARAMATERS FOR Decision Tree , TO CHECK WHICH HYPER PARAMATER IS BEST FOR BOTH\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid= {'criterion' : ['gini' , 'entropy'] , 'max_depth' : [ 2 , 3 , 40  ] , 'min_samples_leaf' : [1,2,4] ,\n",
    "             'max_features' : [None , 'sqrt' , 'log2'] \n",
    "            } \n",
    "grid_search = GridSearchCV( tree , param_grid, cv = 5 , scoring =  'accuracy')\n",
    "\n",
    "#cv stand from cross validation it seperates data into 5 folds then for each fold its tested on the remaining folds\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "print(\"Best Parameters:\" , grid_search.best_params_)\n",
    "print(\"Best Score: \" , grid_search.best_score_)\n",
    "\n",
    "#as you can see we clearly improved the score by tunning bunch of hyperparamaters within the DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "551f3c1a-d987-421b-bdfb-c3b7072491cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "#now lets evaluate another metric recall score\n",
    "#it measures the actuall true that were correctly identified\n",
    "\n",
    "best_clf_rs = grid_search.best_estimator_\n",
    "#this returns the best estimator of the best decison tree we found\n",
    "\n",
    "\n",
    "# predict the labels of test using the best estimator found by the grid search\n",
    "y_pred_rs = best_clf_rs.predict(X_test)\n",
    "\n",
    "# calculate the recall score\n",
    "rs = recall_score(Y_test, y_pred_rs, average='weighted')\n",
    "print(\"Recall Score:\", rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b93b86be-55e1-467a-9c73-31749be41fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12787838345725322\n"
     ]
    }
   ],
   "source": [
    "# NOW FOR REGRESSION\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#the higher the MSE means the farther away the predicted values from the actual values , which is worse \n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X_train , Y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test , y_pred)\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "29139788-0556-47ef-9282-4b670b22bd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree1 0.12787838345725336\n",
      "degree2 0.12496489162542923\n",
      "degree3 0.6011377159647869\n",
      "degree4 67.59509961664598\n"
     ]
    }
   ],
   "source": [
    "#NOW LETS TUNE THE DEGREE OF THE LINEAR REGRESSION TO CHECK WHICH DEGREE IS BEST FOR THE MODEL\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "degrees = [1,2,3,4]\n",
    "\n",
    "for i ,degree in enumerate(degrees , start = 1):\n",
    "    polyF = PolynomialFeatures(degree = degree)\n",
    "    X_train_poly = polyF.fit_transform(X_train)  # we learn from data then transform it to the specific degree\n",
    "    X_test_poly = polyF.transform(X_test)        # since we already learned we use transform only \n",
    "    reg.fit(X_train_poly, Y_train)\n",
    "    y_pred = reg.predict(X_test_poly)\n",
    "    mse = mean_squared_error(Y_test , y_pred)\n",
    "    print(f\"degree{i}\" , mse)\n",
    "\n",
    "#degree 2 is the best since its mse the lowest\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
